{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9248b158",
   "metadata": {},
   "source": [
    "# Данный код я прогонял внутри kaggle, так как на моем ноутбуке это заняло бы больше времени"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15a1de",
   "metadata": {},
   "source": [
    "## Поэтому он не запустится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7739a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pytorchvideo decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ef887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/action-recognition-data/data\"  \n",
    "VIDEOS_DIR = os.path.join(DATA_ROOT, \"raw\")\n",
    "ANN_PATH   = os.path.join(DATA_ROOT, \"processed\", \"annotations_clean.csv\")\n",
    "LABELS_TXT = os.path.join(DATA_ROOT, \"labels.txt\")\n",
    "SPLIT_TRAIN= os.path.join(DATA_ROOT, \"splits\", \"train.txt\")\n",
    "SPLIT_VAL  = os.path.join(DATA_ROOT, \"splits\", \"val.txt\")\n",
    "\n",
    "print(\"VIDEOS_DIR exists:\", os.path.isdir(VIDEOS_DIR))\n",
    "print(\"ANN_PATH exists:\", os.path.isfile(ANN_PATH))\n",
    "print(\"LABELS_TXT exists:\", os.path.isfile(LABELS_TXT))\n",
    "print(\"SPLIT_TRAIN exists:\", os.path.isfile(SPLIT_TRAIN))\n",
    "print(\"SPLIT_VAL exists:\", os.path.isfile(SPLIT_VAL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "def read_labels(labels_path: str):\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = [line.strip() for line in f if line.strip()]\n",
    "    label2id = {l: i for i, l in enumerate(labels)}\n",
    "    return labels, label2id\n",
    "\n",
    "def load_split_list(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def letterbox_batch_to_square(frames_tchw: torch.Tensor, out_size: int = 224) -> torch.Tensor:\n",
    "    T, C, H, W = frames_tchw.shape\n",
    "    scale = out_size / max(H, W)\n",
    "    new_h = int(round(H * scale))\n",
    "    new_w = int(round(W * scale))\n",
    "    resized = F.interpolate(frames_tchw, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    pad_h = out_size - new_h\n",
    "    pad_w = out_size - new_w\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    return F.pad(resized, (pad_left, pad_right, pad_top, pad_bottom), mode=\"constant\", value=0.0)\n",
    "\n",
    "class ActionClipDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ann_csv,\n",
    "        videos_dir,\n",
    "        split_txt,\n",
    "        label2id,\n",
    "        clip_len=32,\n",
    "        out_size=224,\n",
    "        stride=2,\n",
    "        seed=42,\n",
    "        normalize_imagenet=True,\n",
    "        margin_sec: float = 0.5,\n",
    "    ):\n",
    "        self.df = pd.read_csv(ann_csv)\n",
    "        split_videos = set(load_split_list(split_txt))\n",
    "        self.df = self.df[self.df[\"video\"].isin(split_videos)].reset_index(drop=True)\n",
    "        if len(self.df) == 0:\n",
    "            raise ValueError(\"No annotations for this split.\")\n",
    "\n",
    "        self.videos_dir = videos_dir\n",
    "        self.clip_len = int(clip_len)\n",
    "        self.out_size = int(out_size)\n",
    "        self.stride = int(stride)\n",
    "        self.rnd = random.Random(seed)\n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.margin_sec = float(margin_sec)\n",
    "\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        self.std  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "\n",
    "        if \"label_id\" not in self.df.columns:\n",
    "            self.df[\"label_id\"] = self.df[\"label\"].map(label2id).astype(int)\n",
    "\n",
    "    def _get_vr(self, video_name: str):\n",
    "        path = os.path.join(self.videos_dir, video_name)\n",
    "        return VideoReader(path, ctx=cpu(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _safe_bounds_frames(self, vr, start_sec: float, end_sec: float):\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        n_frames = len(vr)\n",
    "\n",
    "        start_f = max(0, int(np.floor(start_sec * fps)))\n",
    "        end_f = min(n_frames - 1, int(np.ceil(end_sec * fps)))\n",
    "\n",
    "        margin_f = int(round(self.margin_sec * fps))\n",
    "        safe_start = start_f + margin_f\n",
    "        safe_end   = end_f - margin_f\n",
    "        if safe_end <= safe_start:\n",
    "            safe_start, safe_end = start_f, end_f\n",
    "\n",
    "        return safe_start, safe_end, fps, n_frames\n",
    "\n",
    "    def get_clip_from_segment(self, video: str, start_sec: float, end_sec: float, start_idx: int | None = None):\n",
    "        \"\"\"\n",
    "        Возвращает клип [3,T,H,W] из сегмента. Если start_idx=None — выбирает случайно.\n",
    "        \"\"\"\n",
    "        vr = self._get_vr(video)\n",
    "        safe_start, safe_end, fps, n_frames = self._safe_bounds_frames(vr, start_sec, end_sec)\n",
    "\n",
    "        need = 1 + (self.clip_len - 1) * self.stride\n",
    "        seg_len = safe_end - safe_start + 1\n",
    "\n",
    "        max_start = safe_start + max(0, seg_len - need)\n",
    "\n",
    "        if start_idx is None:\n",
    "            start_idx = self.rnd.randint(safe_start, max_start) if max_start >= safe_start else safe_start\n",
    "        else:\n",
    "            start_idx = int(np.clip(start_idx, safe_start, max_start if max_start >= safe_start else safe_start))\n",
    "\n",
    "        frame_idxs = [start_idx + i * self.stride for i in range(self.clip_len)]\n",
    "        frame_idxs = [min(i, safe_end) for i in frame_idxs]\n",
    "        frame_idxs = [min(i, n_frames - 1) for i in frame_idxs]\n",
    "\n",
    "        frames = vr.get_batch(frame_idxs).asnumpy()  # [T,H,W,3]\n",
    "        frames = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0  # [T,3,H,W]\n",
    "        frames = letterbox_batch_to_square(frames, self.out_size)\n",
    "\n",
    "        if self.normalize_imagenet:\n",
    "            frames = (frames - self.mean) / self.std\n",
    "\n",
    "        x = frames.permute(1, 0, 2, 3).contiguous()  # [3,T,H,W]\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video = str(row[\"video\"])\n",
    "        start_sec = float(row[\"start_sec\"])\n",
    "        end_sec = float(row[\"end_sec\"])\n",
    "        y = int(row[\"label_id\"])\n",
    "\n",
    "        x = self.get_clip_from_segment(video, start_sec, end_sec, start_idx=None)\n",
    "        return x, torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorchvideo.models.hub import x3d_s\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "labels, label2id = read_labels(LABELS_TXT)\n",
    "num_classes = len(labels)\n",
    "print(\"Classes:\", labels)\n",
    "\n",
    "train_ds = ActionClipDataset(ANN_PATH, VIDEOS_DIR, SPLIT_TRAIN, label2id,\n",
    "                             clip_len=32, out_size=224, stride=2, seed=42, margin_sec=0.5)\n",
    "val_ds   = ActionClipDataset(ANN_PATH, VIDEOS_DIR, SPLIT_VAL,   label2id,\n",
    "                             clip_len=32, out_size=224, stride=2, seed=43, margin_sec=0.5)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "model = x3d_s(pretrained=True)\n",
    "model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # помогает при шумной разметке\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "EPOCHS = 15\n",
    "ACCUM_STEPS = 4                  # эффективный batch ~4\n",
    "VAL_NUM_CLIPS = 5                # multi-clip evaluation: 3/5/7 — разумные значения\n",
    "\n",
    "# OneCycleLR: шаг делаем только когда делаем optimizer.step\n",
    "steps_per_epoch = max(1, len(train_loader) // ACCUM_STEPS)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-4,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.1,\n",
    ")\n",
    "\n",
    "def _val_start_positions(vr, safe_start, safe_end, need, k):\n",
    "    \"\"\"\n",
    "    Равномерные старты по сегменту (детерминированно).\n",
    "    \"\"\"\n",
    "    seg_len = safe_end - safe_start + 1\n",
    "    max_start = safe_start + max(0, seg_len - need)\n",
    "    if max_start <= safe_start or k == 1:\n",
    "        return [safe_start]\n",
    "    return np.linspace(safe_start, max_start, num=k).round().astype(int).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch_multiclip(num_clips=5):\n",
    "    model.eval()\n",
    "    total_loss, correct, n = 0.0, 0, 0\n",
    "\n",
    "    for i in range(len(val_ds.df)):\n",
    "        row = val_ds.df.iloc[i]\n",
    "        video = str(row[\"video\"])\n",
    "        start_sec = float(row[\"start_sec\"])\n",
    "        end_sec = float(row[\"end_sec\"])\n",
    "        y = int(row[\"label_id\"])\n",
    "        y_t = torch.tensor([y], dtype=torch.long, device=device)\n",
    "\n",
    "        # Готовим K клипов детерминированно\n",
    "        vr = val_ds._get_vr(video)\n",
    "        safe_start, safe_end, fps, n_frames = val_ds._safe_bounds_frames(vr, start_sec, end_sec)\n",
    "        need = 1 + (val_ds.clip_len - 1) * val_ds.stride\n",
    "        starts = _val_start_positions(vr, safe_start, safe_end, need, num_clips)\n",
    "\n",
    "        logits_sum = None\n",
    "        for st in starts:\n",
    "            x = val_ds.get_clip_from_segment(video, start_sec, end_sec, start_idx=st)\n",
    "            x = x.unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
    "                logits = model(x)  # [1,C]\n",
    "            logits_sum = logits if logits_sum is None else (logits_sum + logits)\n",
    "\n",
    "        logits_avg = logits_sum / len(starts)\n",
    "\n",
    "        loss = criterion(logits_avg, y_t)\n",
    "        total_loss += loss.item()\n",
    "        pred = int(logits_avg.argmax(dim=1).item())\n",
    "        correct += (pred == y)\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / n, correct / n\n",
    "\n",
    "def train_one_epoch(accum_steps=4):\n",
    "    model.train()\n",
    "    total_loss, correct, n = 0.0, 0, 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    opt_steps = 0\n",
    "\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y) / accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # метрики (на \"реальном\" loss)\n",
    "        b = x.size(0)\n",
    "        total_loss += (loss.item() * accum_steps) * b\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        n += b\n",
    "\n",
    "        if (step + 1) % accum_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            scheduler.step()   # ✅ scheduler step только на optimizer step\n",
    "            opt_steps += 1\n",
    "\n",
    "    # если осталось\n",
    "    if (step + 1) % accum_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        scheduler.step()\n",
    "        opt_steps += 1\n",
    "\n",
    "    return total_loss / n, correct / n\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(accum_steps=ACCUM_STEPS)\n",
    "    va_loss, va_acc = eval_one_epoch_multiclip(num_clips=VAL_NUM_CLIPS)\n",
    "\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch {epoch:02d} | lr {lr_now:.2e} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save({\"model\": model.state_dict(), \"labels\": labels}, \"/kaggle/working/best_x3d_s.pt\")\n",
    "\n",
    "print(\"Best val acc:\", best_val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
